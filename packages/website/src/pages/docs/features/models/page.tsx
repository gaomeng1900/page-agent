import { Fragment } from 'react'

import CodeEditor from '@/components/CodeEditor'
import { useLanguage } from '@/i18n/context'

const BASELINE = new Set([
	'gpt-5.1',
	'claude-haiku-4.5',
	'gemini-3-flash',
	'deepseek-3.2',
	'qwen3-coder-next',
])

// Models grouped by brand, newest first
const MODEL_GROUPS: Record<string, string[]> = {
	OpenAI: ['gpt-5.2', 'gpt-5.1', 'gpt-5', 'gpt-5-mini', 'gpt-4.1', 'gpt-4.1-mini'],
	Google: ['gemini-3-pro', 'gemini-3-flash', 'gemini-2.5'],
	Qwen: ['qwen3-coder-next', 'qwen-3-max', 'qwen-3-plus', 'qwen3:14b (ollama)'],
	DeepSeek: ['deepseek-3.2'],
	Anthropic: [
		'claude-opus-4.6',
		'claude-opus-4.5',
		'claude-sonnet-4.5',
		'claude-haiku-4.5',
		'claude-sonnet-3.5',
	],
	xAI: ['grok-4.1-fast', 'grok-4', 'grok-code-fast'],
	MoonshotAI: ['kimi-k2.5'],
	'Z.AI': ['glm-4.7'],
}

const ModelBadge = ({ model, baseline }: { model: string; baseline?: boolean }) => (
	<div
		className={`px-3 py-1.5 rounded-md text-xs font-medium font-mono transition-colors ${
			baseline
				? 'bg-emerald-500 text-white shadow-sm'
				: 'bg-white/80 dark:bg-gray-800/80 text-gray-800 dark:text-gray-200 border border-gray-300 dark:border-gray-600'
		}`}
	>
		{model}
		{baseline && <span className="ml-1">â­</span>}
	</div>
)

export default function Models() {
	const { isZh } = useLanguage()

	return (
		<div className="max-w-4xl">
			<h1 className="text-4xl font-bold mb-4">{isZh ? 'æ¨¡å‹' : 'Models'}</h1>
			<p className="text-lg text-gray-600 dark:text-gray-400 mb-8">
				{isZh
					? 'å½“å‰æ”¯æŒç¬¦åˆ OpenAI æ¥å£è§„èŒƒä¸”æ”¯æŒ tool call çš„æ¨¡å‹,åŒ…æ‹¬å…¬æœ‰äº‘æœåŠ¡å’Œç§æœ‰éƒ¨ç½²æ–¹æ¡ˆã€‚'
					: 'Supports models that comply with OpenAI API specification and support tool calls, including public cloud services and private deployments.'}
			</p>

			{/* Models Section */}
			<section className="mb-10">
				<h2 className="text-2xl font-semibold mb-3">{isZh ? 'å·²æµ‹è¯•æ¨¡å‹' : 'Tested Models'}</h2>
				<p className="text-sm text-gray-600 dark:text-gray-400 mb-4">
					{isZh
						? 'æ¨èä½¿ç”¨ ToolCall èƒ½åŠ›å¼ºçš„è½»é‡çº§æ¨¡å‹ã€‚'
						: 'Recommended: Fast, lightweight models with strong ToolCall capabilities.'}
				</p>
				<div className="bg-linear-to-br from-emerald-50 to-cyan-50 dark:from-emerald-950/30 dark:to-cyan-950/30 rounded-xl p-6 border border-emerald-200/50 dark:border-emerald-800/50">
					<div className="grid grid-cols-[5rem_1fr] gap-x-3 gap-y-3 items-start">
						{Object.entries(MODEL_GROUPS).map(([brand, models]) => (
							<Fragment key={brand}>
								<span className="text-xs font-semibold text-gray-500 dark:text-gray-400 pt-2">
									{brand}
								</span>
								<div className="flex flex-wrap gap-2">
									{models.map((model) => (
										<ModelBadge key={model} model={model} baseline={BASELINE.has(model)} />
									))}
								</div>
							</Fragment>
						))}
					</div>
				</div>
			</section>

			{/* Tips Section */}
			<section className="mb-10">
				<h2 className="text-2xl font-semibold mb-4">{isZh ? 'æç¤º' : 'Tips'}</h2>
				<div className="p-4 bg-blue-50 dark:bg-blue-950/20 rounded-lg border border-blue-200 dark:border-blue-800">
					<ul className="text-sm text-gray-700 dark:text-gray-300 space-y-2 list-disc pl-5">
						<li>
							{isZh
								? 'ToolCall èƒ½åŠ›è¾ƒå¼±çš„æ¨¡å‹å¯èƒ½è¿”å›é”™è¯¯çš„æ ¼å¼ï¼Œå¸¸è§é”™è¯¯èƒ½å¤Ÿè‡ªåŠ¨æ¢å¤ï¼Œå»ºè®®è®¾ç½®è¾ƒé«˜çš„ temperature'
								: 'Models with weaker ToolCall capabilities may return incorrect formats. Common errors usually auto-recover. Higher temperature recommended'}
						</li>
						<li>
							{isZh
								? 'å°æ¨¡å‹æˆ–è€…æ— æ³•é€‚åº”å¤æ‚ Tool å®šä¹‰çš„æ¨¡å‹ï¼Œé€šå¸¸æ•ˆæœä¸ä½³'
								: 'Small models or those unable to handle complex tool definitions typically perform poorly'}
						</li>
					</ul>
				</div>
			</section>

			{/* Security Section */}
			<section className="mb-10">
				<h2 className="text-2xl font-semibold mb-4">
					{isZh ? 'ğŸ” ç”Ÿäº§ç¯å¢ƒé‰´æƒå»ºè®®' : 'ğŸ” Production Authentication'}
				</h2>
				<div className="bg-yellow-50 dark:bg-yellow-950/20 border-l-4 border-yellow-500 p-5 rounded-r-lg mb-4">
					<p className="text-sm font-semibold text-yellow-900 dark:text-yellow-200">
						{isZh
							? 'âš ï¸ æ°¸è¿œä¸è¦æŠŠçœŸå®çš„ LLM API Key å‘å¸ƒåˆ°å‰ç«¯ä»£ç '
							: 'âš ï¸ Never commit real LLM API Keys to your frontend code'}
					</p>
				</div>
				<div className="bg-gray-50 dark:bg-gray-900/30 rounded-lg p-5 border border-gray-200 dark:border-gray-800">
					<h3 className="font-semibold text-gray-900 dark:text-gray-100 mb-2">
						{isZh ? 'åç«¯ä»£ç†è½¬å‘' : 'Backend Proxy Pattern'}
					</h3>
					<p className="text-sm text-gray-600 dark:text-gray-400 mb-3">
						{isZh
							? 'åœ¨åç«¯æ­å»ºä¸€ä¸ª LLM æµé‡è½¬å‘æ¥å£,è¯¥æ¥å£ä½¿ç”¨ä¸ä½ ç½‘ç«™ä¸Šå…¶ä»–æ¥å£ç›¸åŒçš„é‰´æƒæ–¹å¼,ä¾‹å¦‚:'
							: 'Set up a backend LLM proxy endpoint that uses the same authentication method as other APIs in your website, such as:'}
					</p>
					<ul className="text-sm text-gray-600 dark:text-gray-400 space-y-1">
						<li>{isZh ? 'â€¢ Session/Cookie ä¼šè¯è®¤è¯' : 'â€¢ Session/Cookie-based authentication'}</li>
						<li>
							{isZh ? 'â€¢ OIDC (OpenID Connect) å•ç‚¹ç™»å½•' : 'â€¢ OIDC (OpenID Connect) single sign-on'}
						</li>
						<li>
							{isZh ? 'â€¢ ä¸´æ—¶ Access Key æˆ– JWT Token' : 'â€¢ Temporary Access Key or JWT Token'}
						</li>
					</ul>
				</div>
			</section>

			{/* Configuration Section */}
			<section className="mb-10">
				<h2 className="text-2xl font-semibold mb-4">{isZh ? 'é…ç½®æ–¹å¼' : 'Configuration'}</h2>
				<CodeEditor
					code={`// OpenAI-compatible services (e.g., Alibaba Bailian)
const pageAgent = new PageAgent({
  baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',
  apiKey: 'your-api-key',
  model: 'qwen-plus'
});

// Self-hosted models (e.g., Ollama)
const pageAgent = new PageAgent({
  baseURL: 'http://localhost:11434/v1',
  apiKey: 'NA',
  model: 'qwen3:14b'
});

// Free testing endpoint
// Note: Rate-limited, content-filtered, subject to change. Replace with your own.
// Note: Uses official DeepSeek-chat (3.2). See DeepSeek website for terms & privacy.
const DEMO_MODEL = 'PAGE-AGENT-FREE-TESTING-RANDOM'
const DEMO_BASE_URL = 'https://hwcxiuzfylggtcktqgij.supabase.co/functions/v1/llm-testing-proxy'
const DEMO_API_KEY = 'PAGE-AGENT-FREE-TESTING-RANDOM'
`}
				/>
			</section>

			{/* Ollama Section */}
			<section className="mb-10">
				<h2 className="text-2xl font-semibold mb-4">Ollama</h2>
				<p className="text-sm text-gray-600 dark:text-gray-400 mb-4">
					{isZh
						? 'å·²åœ¨ Ollama 0.15 + qwen3:14b (RTX3090 24GB) ä¸Šæµ‹è¯•é€šè¿‡ã€‚'
						: 'Tested on Ollama 0.15 with qwen3:14b (RTX3090 24GB).'}
				</p>
				<CodeEditor
					code={`LLM_BASE_URL="http://localhost:11434/v1"
LLM_API_KEY="NA"
LLM_MODEL_NAME="qwen3:14b"`}
				/>
				<div className="mt-4 p-4 bg-amber-50 dark:bg-amber-950/20 rounded-lg border border-amber-200 dark:border-amber-800">
					<h3 className="font-semibold text-amber-900 dark:text-amber-200 mb-2">
						{isZh ? 'âš ï¸ æ³¨æ„äº‹é¡¹' : 'âš ï¸ Important Notes'}
					</h3>
					<ul className="text-sm text-gray-700 dark:text-gray-300 space-y-2 list-disc pl-5">
						<li>
							{isZh
								? 'ç¡®ä¿ OLLAMA_ORIGINS è®¾ç½®ä¸º * ä»¥é¿å… 403 é”™è¯¯'
								: 'Add * to OLLAMA_ORIGINS to avoid 403 errors'}
						</li>
						<li>
							{isZh
								? 'å°äº 10B å‚æ•°çš„æ¨¡å‹é€šå¸¸æ•ˆæœä¸ä½³'
								: 'Models smaller than 10B are unlikely to be strong enough'}
						</li>
						<li>{isZh ? 'éœ€è¦æ”¯æŒ tool_call çš„æ¨¡å‹' : 'Requires tool_call capable models'}</li>
						<li>
							{isZh
								? 'ç¡®ä¿ä¸Šä¸‹æ–‡é•¿åº¦å¤§äºè¾“å…¥ token æ•°ï¼Œå¦åˆ™ Ollama ä¼šé™é»˜æˆªæ–­ promptã€‚æ™®é€šé¡µé¢çº¦éœ€ 15k tokenï¼Œéšæ­¥éª¤å¢åŠ ã€‚é»˜è®¤ 4k ä¸Šä¸‹æ–‡é•¿åº¦æ— æ³•æ­£å¸¸å·¥ä½œ'
								: 'Ensure context length exceeds input tokens, or Ollama will silently truncate prompts. ~15k tokens for a typical page, increases with steps. Default 4k context length will NOT work'}
							<ul className="text-sm text-gray-700 dark:text-gray-300 space-y-2 list-disc pl-5">
								<li>
									<code className="text-xs bg-gray-200 dark:bg-gray-700 px-1 rounded">
										$env:OLLAMA_CONTEXT_LENGTH=64000; ollama serve
									</code>
								</li>
							</ul>
						</li>
					</ul>
				</div>
			</section>
		</div>
	)
}
